{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2bits2/recyclable/blob/main/datasetconversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PC_aOs1d1Sj1",
        "outputId": "7e0816f0-76f1-4c66-b5f2-1dc3cf4fbdaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting autodistill-grounding-dino\n",
            "  Downloading autodistill_grounding_dino-0.1.3-py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from autodistill-grounding-dino) (2.1.0+cu121)\n",
            "Collecting autodistill (from autodistill-grounding-dino)\n",
            "  Downloading autodistill-0.1.21-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from autodistill-grounding-dino) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from autodistill-grounding-dino) (4.8.0.76)\n",
            "Collecting rf-groundingdino (from autodistill-grounding-dino)\n",
            "  Downloading rf_groundingdino-0.1.2.tar.gz (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.4/82.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rf-segment-anything (from autodistill-grounding-dino)\n",
            "  Downloading rf_segment_anything-1.0-py3-none-any.whl (36 kB)\n",
            "Collecting supervision (from autodistill-grounding-dino)\n",
            "  Downloading supervision-0.17.1-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autodistill->autodistill-grounding-dino) (4.66.1)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from autodistill->autodistill-grounding-dino) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from autodistill->autodistill-grounding-dino) (6.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autodistill->autodistill-grounding-dino) (8.1.7)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from rf-groundingdino->autodistill-grounding-dino) (0.16.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from rf-groundingdino->autodistill-grounding-dino) (4.35.2)\n",
            "Collecting addict (from rf-groundingdino->autodistill-grounding-dino)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting yapf (from rf-groundingdino->autodistill-grounding-dino)\n",
            "  Downloading yapf-0.40.2-py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.7/254.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm (from rf-groundingdino->autodistill-grounding-dino)\n",
            "  Downloading timm-0.9.12-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from rf-groundingdino->autodistill-grounding-dino) (2.0.7)\n",
            "Requirement already satisfied: matplotlib>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from supervision->autodistill-grounding-dino) (3.7.1)\n",
            "Requirement already satisfied: opencv-python-headless<=4.8.1.78,>=4.5.5.64 in /usr/local/lib/python3.10/dist-packages (from supervision->autodistill-grounding-dino) (4.8.1.78)\n",
            "Requirement already satisfied: scipy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from supervision->autodistill-grounding-dino) (1.11.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->autodistill-grounding-dino) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->autodistill-grounding-dino) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->autodistill-grounding-dino) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->autodistill-grounding-dino) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->autodistill-grounding-dino) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->autodistill-grounding-dino) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->autodistill-grounding-dino) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->supervision->autodistill-grounding-dino) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->supervision->autodistill-grounding-dino) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->supervision->autodistill-grounding-dino) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->supervision->autodistill-grounding-dino) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->supervision->autodistill-grounding-dino) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->supervision->autodistill-grounding-dino) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->supervision->autodistill-grounding-dino) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->autodistill-grounding-dino) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->autodistill-grounding-dino) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm->rf-groundingdino->autodistill-grounding-dino) (0.19.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->rf-groundingdino->autodistill-grounding-dino) (0.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->rf-groundingdino->autodistill-grounding-dino) (2.31.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->rf-groundingdino->autodistill-grounding-dino) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->rf-groundingdino->autodistill-grounding-dino) (0.15.0)\n",
            "Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->rf-groundingdino->autodistill-grounding-dino) (7.0.0)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->rf-groundingdino->autodistill-grounding-dino) (4.1.0)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->rf-groundingdino->autodistill-grounding-dino) (2.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->rf-groundingdino->autodistill-grounding-dino) (3.17.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.5.0->supervision->autodistill-grounding-dino) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->rf-groundingdino->autodistill-grounding-dino) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->rf-groundingdino->autodistill-grounding-dino) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->rf-groundingdino->autodistill-grounding-dino) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->rf-groundingdino->autodistill-grounding-dino) (2023.11.17)\n",
            "Building wheels for collected packages: rf-groundingdino\n",
            "  Building wheel for rf-groundingdino (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rf-groundingdino: filename=rf_groundingdino-0.1.2-py2.py3-none-any.whl size=88734 sha256=a85af2a4d250143eaddd4aba9ab38979aac9eca3ced6ebb6ec1c020488ecf95e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/3f/f8/137b4305e941ad2e0b66cba52e914dda7560308eb91e7e5e47\n",
            "Successfully built rf-groundingdino\n",
            "Installing collected packages: rf-segment-anything, addict, yapf, supervision, timm, autodistill, rf-groundingdino, autodistill-grounding-dino\n",
            "Successfully installed addict-2.4.0 autodistill-0.1.21 autodistill-grounding-dino-0.1.3 rf-groundingdino-0.1.2 rf-segment-anything-1.0 supervision-0.17.1 timm-0.9.12 yapf-0.40.2\n"
          ]
        }
      ],
      "source": [
        "#!pip install ultralytics\n",
        "!pip install autodistill-grounding-dino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtGq_dpz1202",
        "outputId": "228741c3-a610-46dc-b225-6f45ece2be0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting color-transfer\n",
            "  Downloading color_transfer-0.1.tar.gz (1.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: color-transfer\n",
            "  Building wheel for color-transfer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for color-transfer: filename=color_transfer-0.1-py3-none-any.whl size=2368 sha256=5ed6b542e2ce7ffc5a334ba4613d160c0930a2d9c004355f401422b1a3b91a5e\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/84/dd/ae70281354692167c0f4bdd4a231c1b681fb4354124bb7f99d\n",
            "Successfully built color-transfer\n",
            "Installing collected packages: color-transfer\n",
            "Successfully installed color-transfer-0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install color-transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkPObRfa1K60"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "import math\n",
        "from pathlib import Path\n",
        "import os\n",
        "import json\n",
        "import yaml\n",
        "import shutil\n",
        "import glob\n",
        "\n",
        "def get_rotation_matrix(center, angle_degrees):\n",
        "    \"\"\"returns 3x3 matrix for further general processing with non affine transformations\"\"\"\n",
        "    rotation_matrix = cv2.getRotationMatrix2D(center, angle_degrees, 1.0)\n",
        "    rotation_matrix = np.row_stack([rotation_matrix, [0, 0, 1]])\n",
        "    return rotation_matrix\n",
        "\n",
        "def transform_contour(contour, matrix):\n",
        "    \"\"\"applies a transformation matrix on a contour\"\"\"\n",
        "    contour = contour.reshape(-1, 1, 2).astype(np.float32)\n",
        "    contour = cv2.perspectiveTransform(contour, matrix)\n",
        "    contour = contour.astype(np.int32)\n",
        "    return contour\n",
        "\n",
        "def transform_image(image, matrix):\n",
        "    \"\"\"applies a transformation matrix on an image\"\"\"\n",
        "    height, width = image.shape[:2]\n",
        "    return cv2.warpPerspective(image, matrix, (width, height))\n",
        "\n",
        "def get_quad(center, length):\n",
        "    \"\"\" returns a numpy array with 4 point coordinates\"\"\"\n",
        "    return np.array([\n",
        "            center + np.array([-length, -length]),\n",
        "            center + np.array([length, -length]),\n",
        "            center + np.array([-length, length]),\n",
        "            center + np.array([length, length])\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "def blend_images_with_mask(foreground_image, background_image, foreground_mask):\n",
        "    \"\"\" blends foreground and background image together according to the foreground mask\"\"\"\n",
        "    # the values should be\n",
        "    # between 0 and 1\n",
        "    foreground_mask = foreground_mask.astype(float) / 255\n",
        "    background_mask = np.ones(foreground_mask.shape[:2]) - foreground_mask\n",
        "\n",
        "    # to multiply with the images\n",
        "    # we need three channels\n",
        "    foreground_mask = np.dstack(\n",
        "        (foreground_mask,\n",
        "         foreground_mask,\n",
        "         foreground_mask))\n",
        "\n",
        "    background_mask = np.dstack(\n",
        "        (background_mask,\n",
        "         background_mask,\n",
        "         background_mask))\n",
        "\n",
        "    # now we can blend them together\n",
        "    image = (foreground_mask * foreground_image + background_mask * background_image)\n",
        "    image /= (foreground_mask + background_mask)\n",
        "    image = image.astype(dtype=np.uint8)\n",
        "    return image\n",
        "\n",
        "\n",
        "def pack_circles_with_resizing(radii, plane_width, plane_height, scaling_factor=0.83, colliding_threshold=8):\n",
        "    \"\"\"\n",
        "    uniform sampling circle packing algorithm returns resized radii and\n",
        "    randomly placed coordinates in a tuple of arrays (radii, centers)\n",
        "    \"\"\"\n",
        "    radii = radii.copy()\n",
        "    coords = []\n",
        "\n",
        "    num_points= len(radii)\n",
        "    current_radius_index = 0\n",
        "    colliding_count = 0\n",
        "\n",
        "    marker_coords = [\n",
        "        [0, 0],\n",
        "        [0, plane_height],\n",
        "        [plane_width, 0],\n",
        "        [plane_width, plane_height]\n",
        "    ]\n",
        "    marker_radius = 0.104 * plane_width\n",
        "\n",
        "    while len(coords) < num_points:\n",
        "\n",
        "        x = random.randint(0, plane_width)\n",
        "        y = random.randint(0, plane_height)\n",
        "        circle_coord = [x, y]\n",
        "\n",
        "        is_colliding = False\n",
        "\n",
        "        # check collisions\n",
        "        # with markers\n",
        "        for marker_coord in marker_coords:\n",
        "            circle_distance = math.dist(marker_coord, circle_coord)\n",
        "            if circle_distance < radii[current_radius_index] + marker_radius:\n",
        "                is_colliding = True\n",
        "                #print(\"marker colliding\")\n",
        "                break\n",
        "\n",
        "        # check other collisions\n",
        "        if not is_colliding:\n",
        "            for c in range(0, len(coords)):\n",
        "                circle_distance = math.dist(coords[c], circle_coord)\n",
        "                if circle_distance <= radii[current_radius_index] + radii[c]:\n",
        "                    is_colliding = True\n",
        "                    #print(\"colliding\")\n",
        "                    break\n",
        "\n",
        "        if is_colliding:\n",
        "            colliding_count += 1\n",
        "        else:\n",
        "            coords.append([x, y])\n",
        "            current_radius_index += 1\n",
        "            colliding_count = 0\n",
        "\n",
        "        # if the circle is not really\n",
        "        # fitting anywhere we might\n",
        "        # scale the largest circle down\n",
        "        # to get more space\n",
        "        if colliding_count > colliding_threshold:\n",
        "            max_radius_index = np.argmax(radii)\n",
        "            radii[max_radius_index] *= scaling_factor\n",
        "            colliding_count = 0\n",
        "    return (radii, coords)\n",
        "\n",
        "\n",
        "def labelled_contours_to_yolov8_string(image_width, image_height, labelled_contours):\n",
        "    \"\"\"\n",
        "    takes labelled contours and normalizes them to create a string in yolov8 format\n",
        "    \"\"\"\n",
        "    label_strings = []\n",
        "    for labelled_contour in labelled_contours:\n",
        "        label, contour = labelled_contour\n",
        "        # the yolov8 annotation format\n",
        "        # needs for each detection a label\n",
        "        # followed by x y normalized coordinates:\n",
        "        # label x y x y x y x y\n",
        "        normalized_contour = contour / [image_width, image_height]\n",
        "        normalized_contour = normalized_contour.flatten()\n",
        "        contour_str = ' '.join(map(str, normalized_contour))\n",
        "        label_strings.append(f\"{label} {contour_str}\")\n",
        "    label_string = '\\n'.join(label_strings)\n",
        "    return label_string\n",
        "\n",
        "def yolov8_string_to_labelled_contours(image_width, image_height, string):\n",
        "    \"\"\"takes a string in annotation format and returns a labelled contour\"\"\"\n",
        "    labelledContours = []\n",
        "    lines = string.splitlines()\n",
        "    for line in lines:\n",
        "        label, *contour = line.split(' ')\n",
        "        contour = np.asarray(contour, dtype=np.float16)\n",
        "        label = int(label)\n",
        "        contour = contour.reshape(-1,2)\n",
        "        contour *= [image_width, image_height]\n",
        "        contour = np.asarray(contour, dtype=np.int32)\n",
        "        labelledContours.append((label, contour))\n",
        "    return labelledContours\n",
        "\n",
        "\n",
        "def save_image_segmentation(imagesegmentation, image_filename, label_filename):\n",
        "    \"\"\"\n",
        "    saves the image and corresponding annotations to the\n",
        "    specified image and label path in yolov8 format\n",
        "    \"\"\"\n",
        "    image, labelledcontours = imagesegmentation\n",
        "    image_height, image_width = image.shape[:2]\n",
        "    os.makedirs(os.path.dirname(image_filename), exist_ok=True)\n",
        "    os.makedirs(os.path.dirname(label_filename), exist_ok=True)\n",
        "    annotation_content = labelled_contours_to_yolov8_string(image_width, image_height, labelledcontours)\n",
        "    with open(label_filename, \"w\") as f:\n",
        "        f.write(annotation_content)\n",
        "    cv2.imwrite(image_filename, image)\n",
        "\n",
        "\n",
        "def load_image_segmentation(image_filename, label_filename):\n",
        "    \"\"\"\n",
        "    loads an image and all the labelled contours\n",
        "    the label_filename should be a txt file in yolov8 format\n",
        "    \"\"\"\n",
        "    image = cv2.imread(image_filename)\n",
        "    if not os.path.exists(label_filename):\n",
        "        return (image, [])\n",
        "\n",
        "    with open(label_filename, 'r') as f:\n",
        "        string = f.read()\n",
        "    image_height, image_width = image.shape[:2]\n",
        "    labelledContours = yolov8_string_to_labelled_contours(image_width, image_height, string)\n",
        "    return (image, labelledContours)\n",
        "\n",
        "\n",
        "def scale_image_segmentation(imagesegmentation, dst_width, dst_height):\n",
        "    \"\"\" scales the image and all the labelled contours \"\"\"\n",
        "    image, labelledcontours = imagesegmentation\n",
        "    src_height, src_width = image.shape[:2]\n",
        "\n",
        "    # maybe we don't have to do anything\n",
        "    if src_height == dst_height and src_width == dst_width:\n",
        "        return imagesegmentation\n",
        "\n",
        "    # we need to scale the image\n",
        "    # as well as the contours\n",
        "    matrix = cv2.getPerspectiveTransform(\n",
        "        np.array([[0, 0], [src_width, 0], [src_width, src_height], [0, src_height]], dtype=np.float32),\n",
        "        np.array([[0, 0], [dst_width, 0], [dst_width, dst_height], [0, dst_height]], dtype=np.float32))\n",
        "\n",
        "    result_labelled_contours = []\n",
        "    for label, contour in labelledcontours:\n",
        "        contour =  transform_contour(contour, matrix)\n",
        "        result_labelled_contours.append((label, contour))\n",
        "\n",
        "    #image = transform_image(image, matrix)\n",
        "    image = cv2.resize(image, (dst_width, dst_height), interpolation= cv2.INTER_AREA)\n",
        "\n",
        "    result_image = image[0:dst_height, 0:dst_width]\n",
        "    return (result_image, result_labelled_contours)\n",
        "\n",
        "\n",
        "def image_path_to_label_path(image_path):\n",
        "    \"\"\"\n",
        "    for every image in the images folder\n",
        "    there should be txt file in the labels directory\n",
        "    \"\"\"\n",
        "    label_path = \"/labels/\".join(image_path.rsplit(\"/images/\", 1))\n",
        "    label_path = \".txt\".join(label_path.rsplit(\".jpg\"))\n",
        "    return label_path\n",
        "\n",
        "def get_random_color():\n",
        "    \"\"\" returns a tuple with random RGB values \"\"\"\n",
        "    return tuple(np.random.random(size=3) * 255)\n",
        "\n",
        "\n",
        "def object_mask_to_foreground_mask(object_mask):\n",
        "    \"\"\"\n",
        "    modifies the mask of an object for image blending\n",
        "    \"\"\"\n",
        "    mask = object_mask.copy()\n",
        "    #kernel = np.ones((8,8),np.uint8)\n",
        "    #mask = cv2.dilate(mask, kernel, iterations = 2)\n",
        "    #mask = cv2.GaussianBlur(mask, (51, 51), 0)\n",
        "    #maskmo = cv2.distanceTransform(mask, cv2.DIST_L2, maskSize=3, dstType=cv2.CV_8U)\n",
        "    #maskmo = cv2.normalize(maskmo, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def image_segmentations_to_labels_masks_images(image_segmentations):\n",
        "    \"\"\"\n",
        "    unfolds the image and labelled contours into seperate\n",
        "    labels images and masks for further processing\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    images = []\n",
        "    masks = []\n",
        "    for image_segmentation in image_segmentations:\n",
        "        image, labelledcontours = image_segmentation\n",
        "        for label, contour in labelledcontours:\n",
        "            imgcpy = image.copy()\n",
        "            mask = np.zeros(imgcpy.shape[:2], dtype=np.uint8)\n",
        "            cv2.drawContours(mask, [contour], 0, 255, -1)\n",
        "            masks.append(mask)\n",
        "            images.append(imgcpy)\n",
        "            labels.append(label)\n",
        "    return (labels, masks, images)\n",
        "\n",
        "\n",
        "def apply_transform_on_labels_masks_images(labelsmasksimages, matrices):\n",
        "    \"\"\" applies a transformation matrix on masks and images \"\"\"\n",
        "    labels, masks, images = labelsmasksimages\n",
        "    for i in range(0, len(masks)):\n",
        "        matrix = matrices[i]\n",
        "        masks[i] = transform_image(masks[i], matrix)\n",
        "        images[i] = transform_image(images[i], matrix)\n",
        "    return (labels, masks, images)\n",
        "\n",
        "\n",
        "def labels_masks_images_to_image_segmentations(labelsmasksimages, background, objectMask2ForegroundMask):\n",
        "    labels, masks, images = labelsmasksimages\n",
        "\n",
        "    # it is very unlikely\n",
        "    # but maybe some masks\n",
        "    # don't contain visible\n",
        "    # contours anymore\n",
        "    # so we have to filter\n",
        "    # those out\n",
        "    object_contours = []\n",
        "    object_masks = []\n",
        "    object_labels = []\n",
        "    object_images = []\n",
        "\n",
        "    for i in range(0, len(masks)):\n",
        "        # we can assume that there is only\n",
        "        # one contour in the new image\n",
        "        mask = masks[i]\n",
        "        contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        if len(contours) == 0:\n",
        "            print(\"filtered contour\")\n",
        "            continue\n",
        "        contour = contours[0]\n",
        "        object_contours.append(contour)\n",
        "        object_masks.append(mask)\n",
        "        object_labels.append(labels[i])\n",
        "        object_images.append(images[i])\n",
        "\n",
        "    # blend images together\n",
        "    result_image = background.copy()\n",
        "    for i in range(0, len(object_masks)):\n",
        "        foreground_mask = objectMask2ForegroundMask(object_masks[i])\n",
        "        result_image = blend_images_with_mask(object_images[i], result_image, foreground_mask)\n",
        "\n",
        "    result_labelled_contours = list(zip(object_labels, object_contours))\n",
        "    return (result_image, result_labelled_contours)\n",
        "\n",
        "\n",
        "def radii_centers_to_transformation_matrices(src_radii, src_centers, image_width, image_height):\n",
        "    matrices = []\n",
        "    dst_radii, dst_centers = pack_circles_with_resizing(\n",
        "        src_radii,\n",
        "        image_width,\n",
        "        image_height\n",
        "    )\n",
        "    for i in range(0, len(dst_centers)):\n",
        "        src = get_quad(src_centers[i], src_radii[i])\n",
        "        dst = get_quad(dst_centers[i], dst_radii[i])\n",
        "        movematrix = cv2.getPerspectiveTransform(src, dst)\n",
        "        angle_degrees = random.randint(0, 359)\n",
        "        rotationmatrix = get_rotation_matrix(dst_centers[i], angle_degrees)\n",
        "        matrices.append(np.matmul(rotationmatrix, movematrix))\n",
        "    return matrices\n",
        "\n",
        "\n",
        "def image_segmentations_to_radii_centers(imagesegmentations):\n",
        "    centers = []\n",
        "    radii = []\n",
        "    for image, labelledcontours in imagesegmentations:\n",
        "        for label, contour in labelledcontours:\n",
        "            center, radius = cv2.minEnclosingCircle(contour)\n",
        "            centers.append(center)\n",
        "            radii.append(radius)\n",
        "    return (radii, centers)\n",
        "\n",
        "\n",
        "def visualize_image_segmentation(image_segmentation, opacity=0.8):\n",
        "    img, labelled_contours = image_segmentation\n",
        "    image = img.copy()\n",
        "    overlay = image.copy()\n",
        "    for i, (label, contour) in enumerate(labelled_contours):\n",
        "        cv2.drawContours(image, [contour], 0, get_random_color(), 3)\n",
        "        font = cv2.FONT_HERSHEY_DUPLEX\n",
        "        cx, cy, w, h = cv2.boundingRect(contour)\n",
        "        cx += int(0.5 * w)\n",
        "        cy += int(0.5 * h)\n",
        "        cv2.putText(image, str(label), (cx, cy), font, 0.6, (0, 0, 0), 1, cv2.LINE_AA)\n",
        "    image = cv2.addWeighted(image, opacity, overlay, 1.0 - opacity, 0)\n",
        "    return image\n",
        "\n",
        "\n",
        "# solution found here:\n",
        "# https://stackoverflow.com/questions/62941378/how-to-sort-glob-glob-numerically\n",
        "file_pattern = re.compile(r'.*?(\\d+).*?')\n",
        "def get_numerical_order(file):\n",
        "    match = file_pattern.match(Path(file).name)\n",
        "    if not match:\n",
        "        return math.inf\n",
        "    return int(match.groups()[0])\n",
        "\n",
        "def get_all_images_in_numerical_order(dirpath):\n",
        "    dirpath = os.path.abspath(dirpath)\n",
        "    return sorted(glob.glob(f\"{dirpath}/**/*.jpg\", recursive=True), key=get_numerical_order)\n",
        "\n",
        "\n",
        "def load_or_init_checkpoint(checkpoint_path, data):\n",
        "    try:\n",
        "        f = open(checkpoint_path, \"r\")\n",
        "        checkpoint = json.load(f)\n",
        "        f.close()\n",
        "    except:\n",
        "        print(f\"couldnt open {checkpoint_path}\")\n",
        "        checkpoint = data\n",
        "    return checkpoint\n",
        "\n",
        "def save_checkpoint(checkpoint_path, checkpoint):\n",
        "    try:\n",
        "        f = open(checkpoint_path, \"w\")\n",
        "        json.dump(checkpoint, f)\n",
        "        f.close()\n",
        "    except IOError as e:\n",
        "        print(e)\n",
        "\n",
        "def stitch_image_segmentations_together(unscaled_imagesegmentations, background):\n",
        "    image_height, image_width = background.shape[:2]\n",
        "    image_segmentations = []\n",
        "    for imgseg in unscaled_imagesegmentations:\n",
        "        image_segmentations.append(scale_image_segmentation(imgseg, image_width, image_height))\n",
        "\n",
        "    labelsmasksimages = image_segmentations_to_labels_masks_images(image_segmentations)\n",
        "    src_radii, src_centers = image_segmentations_to_radii_centers(image_segmentations)\n",
        "    matrices = radii_centers_to_transformation_matrices(src_radii, src_centers, image_width, image_height)\n",
        "    apply_transform_on_labels_masks_images(labelsmasksimages, matrices)\n",
        "    newimagesegmentation = labels_masks_images_to_image_segmentations(labelsmasksimages, background,\n",
        "                                                                      object_mask_to_foreground_mask)\n",
        "    return newimagesegmentation\n",
        "\n",
        "def image_paths_to_image_segmentations(image_paths, only_biggest_contour=True):\n",
        "    image_segmentations = []\n",
        "    for image_path in image_paths:\n",
        "        annotation_path = image_path_to_label_path(image_path)\n",
        "        try:\n",
        "            image_segmentation = load_image_segmentation(image_path, annotation_path)\n",
        "        except IOError as e:\n",
        "            print(f\"warning: propably missing annotation for image {image_path}. annotation_path: {annotation_path}\")\n",
        "            continue\n",
        "        if only_biggest_contour:\n",
        "            image, labelledcontours = image_segmentation\n",
        "            if len(labelledcontours) == 0:\n",
        "                continue\n",
        "            labelledcontours = [sorted(labelledcontours, key=lambda labelcontour: cv2.contourArea(labelcontour[1]))[0]]\n",
        "            image_segmentation = (image, labelledcontours)\n",
        "        image_segmentations.append(image_segmentation)\n",
        "    return image_segmentations\n",
        "\n",
        "# Somehow I found the value of `gamma=1.2` to be the best in my case\n",
        "def adjust_gamma(image, gamma=1.2):\n",
        "    # build a lookup table mapping the pixel values [0, 255] to\n",
        "    # their adjusted gamma values\n",
        "    invGamma = 1.0 / gamma\n",
        "    table = np.array([((i / 255.0) ** invGamma) * 255\n",
        "        for i in np.arange(0, 256)]).astype(\"uint8\")\n",
        "\n",
        "    # apply gamma correction using the lookup table\n",
        "    return cv2.LUT(image, table)\n",
        "\n",
        "def get_simple_background_provider(directory):\n",
        "    image_paths = glob.glob(f\"{os.path.abspath(directory)}/**/*.jpg\", recursive=True)\n",
        "    def get_random_background():\n",
        "        i = random.randrange(len(image_paths))\n",
        "        image_path = image_paths[i]\n",
        "        image = cv2.imread(image_path)\n",
        "        image = adjust_gamma(image, gamma=1.0 + random.uniform(0.2, 0.9))\n",
        "        h, w = image.shape[:2]\n",
        "        rot_mat = get_rotation_matrix((w/2, h/2), random.randrange(0, 90))\n",
        "        image = cv2.warpPerspective(image, rot_mat, (w, h))\n",
        "        return image\n",
        "    return get_random_background\n",
        "\n",
        "\n",
        "\n",
        "import color_transfer\n",
        "def contrast_brightness_transfer(source, target):\n",
        "    source = cv2.cvtColor(source, cv2.COLOR_BGR2HSV).astype(np.float32)\n",
        "    target = cv2.cvtColor(target, cv2.COLOR_BGR2HSV).astype(np.float32)\n",
        "    h_mean_src, h_std_src, s_mean_src, s_std_src, v_mean_src, v_std_src =  color_transfer.image_stats(source)\n",
        "    h_mean_dst, h_std_dst, s_mean_dst, s_std_dst, v_mean_dst, v_std_dst =  color_transfer.image_stats(target)\n",
        "    h, s, v = cv2.split(target)\n",
        "    v -= v_mean_dst\n",
        "    v = ( v_std_dst / v_std_src) * v\n",
        "    v += v_mean_src\n",
        "    v = np.clip(v, 0, 255)\n",
        "    transfer = cv2.merge([h, s, v])\n",
        "    transfer = cv2.cvtColor(transfer.astype(np.uint8), cv2.COLOR_HSV2BGR)\n",
        "    return transfer\n",
        "\n",
        "\n",
        "def seg_augment(src_dir, dst_dir, get_background, max_object_count, image_prefix=\"\", seed=3):\n",
        "    \"\"\"generates a new dataset with just augmented image segmentations\"\"\"\n",
        "    src_dir = os.path.abspath(src_dir)\n",
        "    dst_dir = os.path.abspath(dst_dir)\n",
        "\n",
        "    os.makedirs(f\"{dst_dir}/labels\", exist_ok=True)\n",
        "    os.makedirs(f\"{dst_dir}/images\", exist_ok=True)\n",
        "\n",
        "    checkpoint_path = f\"{dst_dir}/checkpoint.json\"\n",
        "    checkpoint = load_or_init_checkpoint(checkpoint_path, {\"seed\": seed, \"it\":0, \"max_object_count\": max_object_count})\n",
        "    max_object_count = checkpoint[\"max_object_count\"]\n",
        "\n",
        "    image_paths = get_all_images_in_numerical_order(src_dir)\n",
        "    random.seed(checkpoint[\"seed\"])\n",
        "    random.shuffle(image_paths)\n",
        "    selections = []\n",
        "    i = 0\n",
        "    while i < len(image_paths):\n",
        "        num_selected_images = random.randint(1, max_object_count)\n",
        "        selections.append((i, i+num_selected_images))\n",
        "        i += num_selected_images\n",
        "\n",
        "    print(f\"{len(selections)} images to generate\")\n",
        "    while checkpoint[\"it\"] < len(selections):\n",
        "        start, end = selections[checkpoint[\"it\"]]\n",
        "        selected_image_paths = image_paths[start:end]\n",
        "        background = get_background()\n",
        "        image_segmentations = image_paths_to_image_segmentations(selected_image_paths)\n",
        "        for i in range(len(image_segmentations)):\n",
        "            image, labelled_contours = image_segmentations[i]\n",
        "            image = contrast_brightness_transfer(background, image)\n",
        "            image_segmentations[i] = (image, labelled_contours)\n",
        "\n",
        "        newimagesegmentation = stitch_image_segmentations_together(image_segmentations, background)\n",
        "\n",
        "        if bool(random.getrandbits(1)):\n",
        "            image, labelled_contours = newimagesegmentation\n",
        "            smooth_kernel_size = random.choice([(3, 3), (5,5)])\n",
        "            image = cv2.GaussianBlur(image, smooth_kernel_size, 0)\n",
        "            newimagesegmentation = (image, labelled_contours)\n",
        "\n",
        "        output_image_path = f\"{dst_dir}/images/{image_prefix}image_{start}_{end}.jpg\"\n",
        "        output_label_path = image_path_to_label_path(output_image_path)\n",
        "        save_image_segmentation(newimagesegmentation, output_image_path, output_label_path)\n",
        "        checkpoint[\"it\"] += 1\n",
        "        save_checkpoint(checkpoint_path, checkpoint)\n",
        "    print(\"done\")\n",
        "\n",
        "\n",
        "def seg_remap(src_dir, dst_dir, remap, remove_backgrounds=False):\n",
        "    \"\"\"outputs the same segmentation dataset with remapped labels. all labels not specified are dropped.\"\"\"\n",
        "    # this calculation is done\n",
        "    # because we want to continue\n",
        "    # where we left of\n",
        "    # in case this function was interrupted\n",
        "    relative_image_paths = [ os.path.relpath(p, src_dir) for p in get_all_images_in_numerical_order(src_dir)]\n",
        "    relative_image_paths_processed = [os.path.relpath(p, dst_dir) for p in get_all_images_in_numerical_order(dst_dir)]\n",
        "    relative_image_paths_not_processed = list(set(relative_image_paths) - set(relative_image_paths_processed))\n",
        "\n",
        "    for relative_image_path in relative_image_paths_not_processed:\n",
        "        image_path = f\"{src_dir}/{relative_image_path}\"\n",
        "        label_path = image_path_to_label_path(image_path)\n",
        "        image_segmentation = load_image_segmentation(image_path, label_path)\n",
        "        image, labelledcontours = image_segmentation\n",
        "\n",
        "        relabelledcontours = []\n",
        "        skip_image = False\n",
        "        for label, contour in labelledcontours:\n",
        "            if label not in remap:\n",
        "                skip_image = True\n",
        "                break\n",
        "            newlabel = remap[label]\n",
        "            relabelledcontours.append((newlabel, contour))\n",
        "\n",
        "        if remove_backgrounds and len(labelledcontours) == 0:\n",
        "            skip_image = True\n",
        "\n",
        "        if skip_image:\n",
        "            continue\n",
        "\n",
        "        new_image_segmentation = (image, relabelledcontours)\n",
        "\n",
        "        output_image_path = image_path.replace(src_dir, dst_dir)\n",
        "        output_label_path = image_path_to_label_path(output_image_path)\n",
        "        save_image_segmentation(new_image_segmentation, output_image_path, output_label_path)\n",
        "\n",
        "def get_yaml_content(yaml_file):\n",
        "    info = None\n",
        "    with open(yaml_file, \"r\") as f:\n",
        "        try:\n",
        "            info = yaml.safe_load(f)\n",
        "        except yaml.YAMLError as exc:\n",
        "            print(exc)\n",
        "            return None\n",
        "    return info\n",
        "\n",
        "\n",
        "def get_yaml_name_remap(yaml_file, dstName2Category):\n",
        "    with open(yaml_file, \"r\") as f:\n",
        "        try:\n",
        "            info = yaml.safe_load(f)\n",
        "        except yaml.YAMLError as exc:\n",
        "            print(exc)\n",
        "            return None\n",
        "\n",
        "    srcName2Category = {}\n",
        "    for i, name in enumerate(info[\"names\"]):\n",
        "        srcName2Category[name] = i\n",
        "\n",
        "    category2Category = {}\n",
        "    for name in dstName2Category:\n",
        "        category2Category[srcName2Category[name]] = dstName2Category[name]\n",
        "    return category2Category\n",
        "\n",
        "\n",
        "def seg_name_remap(src_dir, dst_dir, name_category_map, remove_backgrounds=False):\n",
        "    src_dir = os.path.abspath(src_dir)\n",
        "    dst_dir = os.path.abspath(dst_dir)\n",
        "    data_yaml_path = f\"{src_dir}/data.yaml\"\n",
        "    category_remap = get_yaml_name_remap(data_yaml_path, name_category_map)\n",
        "    seg_remap(src_dir, dst_dir, category_remap, remove_backgrounds)\n",
        "\n",
        "\n",
        "def combine_and_split_segs(seg_paths, output_path, weights=(0.7, 0.2, 0.1)):\n",
        "    output_path = os.path.abspath(output_path)\n",
        "    possible_subdirs = [\"train\", \"val\", \"test\"]\n",
        "\n",
        "    for subdir in possible_subdirs:\n",
        "        output_label_dir = f\"{output_path}/{subdir}/labels/\"\n",
        "        output_images_dir = f\"{output_path}/{subdir}/images/\"\n",
        "        os.makedirs(output_label_dir, exist_ok=True)\n",
        "        os.makedirs(output_images_dir, exist_ok=True)\n",
        "\n",
        "    seg_paths = list(map(os.path.abspath, seg_paths))\n",
        "    for seg_path in seg_paths:\n",
        "        basename = os.path.basename(seg_path)\n",
        "        image_paths = get_all_images_in_numerical_order(seg_path)\n",
        "\n",
        "        # choose a folder for each image\n",
        "        image_subdirs = random.choices(possible_subdirs, weights=weights, k=len(image_paths))\n",
        "\n",
        "        for i in range(0, len(image_paths)):\n",
        "            input_image_path = image_paths[i]\n",
        "            input_label_path = image_path_to_label_path(input_image_path)\n",
        "\n",
        "            output_image_path = f\"{output_path}/{image_subdirs[i]}/images/{basename}_image_{i}.jpg\"\n",
        "            output_label_path = image_path_to_label_path(output_image_path)\n",
        "\n",
        "            shutil.copyfile(input_label_path, output_label_path)\n",
        "            shutil.copyfile(input_image_path, output_image_path)\n",
        "\n",
        "\n",
        "def save_standard_yaml(dst_yaml_path, category_label_map):\n",
        "    # sort by label numbers\n",
        "    category_label_map = dict(sorted(category_label_map.items(), key=lambda item: item[1]))\n",
        "\n",
        "    os.makedirs(os.path.dirname(dst_yaml_path), exist_ok=True)\n",
        "\n",
        "    info = {}\n",
        "    info[\"names\"] = [key for key in category_label_map]\n",
        "\n",
        "    with open(dst_yaml_path, \"w\") as f:\n",
        "        yaml.dump(info, f, default_flow_style=False)\n",
        "\n",
        "\n",
        "def combine_segs(datasetname2names, output_path, name2intcategory, weights,\n",
        "                 get_background=None, max_object_count=3, seed=3):\n",
        "    output_path = os.path.abspath(output_path)\n",
        "    save_standard_yaml(f\"{output_path}/data.yaml\", name2intcategory)\n",
        "\n",
        "    dataset_name_category_mapping = {}\n",
        "    for datasetname in datasetname2names:\n",
        "        name2categorymap = {}\n",
        "        for datasetcategoryname in datasetname2names[datasetname]:\n",
        "            name2categorymap[datasetcategoryname] = name2intcategory[datasetname2names[datasetname][datasetcategoryname]]\n",
        "        dataset_name_category_mapping[datasetname] = name2categorymap\n",
        "\n",
        "\n",
        "    remapped_dataset_names = {}\n",
        "    for datasetname in dataset_name_category_mapping:\n",
        "        remapped_dataset_names[datasetname] = f\"{output_path}/remapped_{os.path.basename(datasetname)}\"\n",
        "\n",
        "    augmented_dataset_names = {}\n",
        "    for datasetname in remapped_dataset_names:\n",
        "        augmented_dataset_names[datasetname] = f\"{output_path}/augmented_{os.path.basename(datasetname)}\"\n",
        "\n",
        "    for dataset_path in remapped_dataset_names:\n",
        "         seg_name_remap(\n",
        "             dataset_path,\n",
        "             remapped_dataset_names[dataset_path],\n",
        "             dataset_name_category_mapping[dataset_path])\n",
        "\n",
        "    generated_datasets = list(remapped_dataset_names.values())\n",
        "\n",
        "    if get_background is not None:\n",
        "        for dataset_path in augmented_dataset_names:\n",
        "            seg_augment(dataset_path,\n",
        "                        augmented_dataset_names[dataset_path],\n",
        "                        get_background,\n",
        "                        max_object_count,\n",
        "                        image_prefix=f\"{os.path.basename(dataset_path)}_\",\n",
        "                        seed=seed)\n",
        "            generated_datasets.append(augmented_dataset_names[dataset_path])\n",
        "\n",
        "    combine_and_split_segs(generated_datasets, output_path)\n",
        "\n",
        "\n",
        "def cutout_quad(image, quad):\n",
        "    image_height, image_width = image.shape[:2]\n",
        "    p1 = quad[0]\n",
        "    p2 = quad[1]\n",
        "    p3 = quad[2]\n",
        "    y1 = int(p1[1])\n",
        "    y2 = int(p3[1])\n",
        "    x1 = int(p1[0])\n",
        "    x2 = int(p2[0])\n",
        "    y1 = max(0, y1)\n",
        "    y2 = min(image_height, y2)\n",
        "    x1 = max(0, x1)\n",
        "    x2 = min(image_width, x2)\n",
        "    return image[y1:y2, x1:x2]\n",
        "\n",
        "\n",
        "def cutout_single_image_segmentation(image, masks, labels):\n",
        "    imagesegmentations = []\n",
        "    for label, mask in zip(labels, masks):\n",
        "\n",
        "\n",
        "        contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        if len(contours) == 0:\n",
        "            continue\n",
        "\n",
        "        contour = contours[0]\n",
        "\n",
        "\n",
        "        kernel = np.ones((10,10),np.uint8)\n",
        "        mask_dilated = cv2.dilate(mask, kernel, iterations = 3)\n",
        "        contours_dilated, _ = cv2.findContours(mask_dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        if len(contours_dilated) == 0:\n",
        "            continue\n",
        "        contour_dilated = contours_dilated[0]\n",
        "\n",
        "        x, y, w, h = cv2.boundingRect(contour_dilated)\n",
        "\n",
        "        if cv2.contourArea(contour) < 100:\n",
        "            continue\n",
        "\n",
        "        src = np.array([\n",
        "            [x, y],\n",
        "            [x+w, y],\n",
        "            [x+w, y+h],\n",
        "            [x, y+h]], dtype=np.float32)\n",
        "\n",
        "        image_cutout = cutout_quad(image, src)\n",
        "        mask_cutout =  cutout_quad(mask, src)\n",
        "\n",
        "        contours, _ = cv2.findContours(mask_cutout, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        if len(contours) == 0:\n",
        "            continue\n",
        "        contour = contours[0]\n",
        "        imagesegmentations.append((image_cutout, [(label, contour)]))\n",
        "    return imagesegmentations\n",
        "\n",
        "\n",
        "\n",
        "############ GROUNDING DINO + SAM ##########\n",
        "\n",
        "from typing import List\n",
        "import torch\n",
        "from groundingdino.util.inference import Model\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "def enhanceClassName(class_names: List[str]) -> List[str]:\n",
        "    return [\n",
        "        f\"all {class_name}s\"\n",
        "        for class_name\n",
        "        in class_names\n",
        "    ]\n",
        "\n",
        "def segment(sam_predictor: SamPredictor, image: np.ndarray, xyxy: np.ndarray) -> np.ndarray:\n",
        "    sam_predictor.set_image(image)\n",
        "    result_masks = []\n",
        "    for box in xyxy:\n",
        "        masks, scores, logits = sam_predictor.predict(\n",
        "            box=box,\n",
        "            multimask_output=True\n",
        "        )\n",
        "        index = np.argmax(scores)\n",
        "        result_masks.append(masks[index])\n",
        "    return np.array(result_masks)\n",
        "\n",
        "\n",
        "\n",
        "def cls2seg(grounding_dino_model,\n",
        "            sam_predictor,\n",
        "            src_dir,\n",
        "            dst_dir,\n",
        "            category_to_label_map,\n",
        "            box_threshold = 0.35,\n",
        "            text_threshold = 0.25):\n",
        "\n",
        "\n",
        "    # normalize paths\n",
        "    src_dir = os.path.abspath(src_dir)\n",
        "    dst_dir = os.path.abspath(dst_dir)\n",
        "\n",
        "    dst_yaml_path = f\"{dst_dir}/data.yaml\"\n",
        "\n",
        "    save_standard_yaml(dst_yaml_path, category_to_label_map)\n",
        "\n",
        "    image_paths = get_all_images_in_numerical_order(src_dir)\n",
        "\n",
        "    dst_checkpoint_path = f\"{dst_dir}/checkpoint.txt\"\n",
        "    try:\n",
        "        f = open(dst_checkpoint_path, \"r\")\n",
        "        last_image_path = f.read()\n",
        "        f.close()\n",
        "        skip = True\n",
        "    except IOError:\n",
        "        skip = False\n",
        "\n",
        "    for image_path in image_paths:\n",
        "        if skip:\n",
        "            if image_path == last_image_path:\n",
        "                skip = False\n",
        "            continue\n",
        "\n",
        "        splitted_path = os.path.normpath(image_path).split(os.sep)\n",
        "\n",
        "        image_name = splitted_path[-1]\n",
        "        category = splitted_path[-2]\n",
        "        train_val_or_test = splitted_path[-3]\n",
        "\n",
        "        dst_image_name = f\"{dst_dir}/{train_val_or_test}/images/{image_name}\"\n",
        "        dst_label_name = image_path_to_label_path(dst_image_name)\n",
        "\n",
        "        # check if it is already processed\n",
        "        if os.path.exists(dst_image_name):\n",
        "            continue\n",
        "\n",
        "        # skip those which are not specified\n",
        "        if category not in category_to_label_map:\n",
        "            continue\n",
        "\n",
        "        label = category_to_label_map[category]\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            continue\n",
        "\n",
        "        image_height, image_width = image.shape[:2]\n",
        "\n",
        "        # detect objects\n",
        "        # with grounding dino\n",
        "        detections = grounding_dino_model.predict_with_classes(\n",
        "            image=image,\n",
        "            classes=enhanceClassName(class_names=[category]),\n",
        "                 box_threshold=box_threshold,\n",
        "                 text_threshold=text_threshold\n",
        "          )\n",
        "\n",
        "        with open(dst_checkpoint_path, \"w\") as f:\n",
        "            print(f\"{image_path}\")\n",
        "            f.write(image_path)\n",
        "\n",
        "        if len(detections) == 0:\n",
        "            continue\n",
        "\n",
        "        # convert detections\n",
        "        # to masks with Sam\n",
        "        detections.mask = segment(\n",
        "            sam_predictor=sam_predictor,\n",
        "            image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB),\n",
        "            xyxy=detections.xyxy\n",
        "        )\n",
        "\n",
        "        masks = []\n",
        "        confs = []\n",
        "        areas = []\n",
        "        for xyxy, mask, confidence, class_id, _ in detections:\n",
        "            masks.append(mask.astype(dtype='uint8') * 255)\n",
        "            confs.append(confidence)\n",
        "            areas.append(np.count_nonzero(mask))\n",
        "\n",
        "        mask_index = np.argmax(confs)\n",
        "        mask = masks[mask_index]\n",
        "\n",
        "        imagesegmentations = cutout_single_image_segmentation(image, [mask], [label])\n",
        "        if len(imagesegmentations) == 0:\n",
        "            continue\n",
        "\n",
        "        save_image_segmentation(imagesegmentations[0], dst_image_name, dst_label_name)\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "def load_segmentation_ground_truth(image_dimension, annotation_path, isbox):\n",
        "    \"\"\"loads contours / masks / labels and boxes from an yolov8 annotation txt file\"\"\"\n",
        "    with open(annotation_path, 'r') as f:\n",
        "        lines = f.read().splitlines()\n",
        "\n",
        "    image_height, image_width = image_dimension\n",
        "\n",
        "    # detected boxes with format\n",
        "    # [xmin, ymin, xmax, ymax]\n",
        "    # in absolute image coordinates\n",
        "    true_boxes = []\n",
        "\n",
        "    # boolean mask of\n",
        "    # the image per category\n",
        "    true_masks = []\n",
        "    true_labels = []\n",
        "    true_names = []\n",
        "    true_contours = []\n",
        "\n",
        "    # extract truth values\n",
        "    # from annotation path\n",
        "    for line in lines:\n",
        "\n",
        "        # extract info\n",
        "        label, *contour = line.split(' ')\n",
        "        contour = np.asarray(contour, dtype=np.float16)\n",
        "        label = int(label)\n",
        "\n",
        "        # convert to\n",
        "        # correct format\n",
        "        if isbox:\n",
        "            cx = contour[0] * image_width\n",
        "            cy = contour[1] * image_height\n",
        "            w  = contour[2] * image_width\n",
        "            h  = contour[3] * image_height\n",
        "            xoffset = w/2\n",
        "            yoffset = h/2\n",
        "            contour = np.array([\n",
        "                [cx-xoffset, cy-yoffset],\n",
        "                [cx+xoffset, cy-yoffset],\n",
        "                [cx+xoffset, cy+yoffset],\n",
        "                [cx-xoffset, cy+yoffset],\n",
        "            ])\n",
        "            box = [cx-xoffset, cy-yoffset, cx+xoffset, cy+yoffset]\n",
        "\n",
        "        else:\n",
        "            contour = contour.reshape(-1,2)\n",
        "            contour *= [image_width, image_height]\n",
        "            contour = np.asarray(contour, dtype=np.int32)\n",
        "            box_x,box_y,box_w,box_h = cv2.boundingRect(contour)\n",
        "            box = [box_x, box_y, box_x + box_w, box_y + box_h]\n",
        "\n",
        "        mask = np.zeros((image_height, image_width), np.uint8)\n",
        "        cv2.drawContours(mask, [contour.astype('int')], 0, 255, -1)\n",
        "\n",
        "        true_contours.append(contour)\n",
        "        true_masks.append(mask)\n",
        "        true_labels.append(label)\n",
        "        true_boxes.append(box)\n",
        "\n",
        "    return {\n",
        "        \"boxes\": true_boxes,\n",
        "        \"labels\": true_labels,\n",
        "        \"masks\": true_masks,\n",
        "        \"contours\": true_contours\n",
        "    }\n",
        "\n",
        "\n",
        "def objectdet2seg(\n",
        "    sam_predictor,\n",
        "    object_detection_directory,\n",
        "    output_segmentation_directory,\n",
        "    box_threshold = 0.35,\n",
        "    text_threshold = 0.25,\n",
        "    standard_size=[224, 224]):\n",
        "  \"\"\" converts an object detection dataset into a segmentation dataset using segment anything model\"\"\"\n",
        "\n",
        "  object_detection_directory = os.path.abspath(object_detection_directory)\n",
        "  output_segmentation_directory = os.path.abspath(output_segmentation_directory)\n",
        "\n",
        "  # we expect a data.yaml file in the directory\n",
        "  # containing some nice configuration data\n",
        "  expected_data_yaml_file = f\"{object_detection_directory}/data.yaml\"\n",
        "  with open(expected_data_yaml_file, \"r\") as f:\n",
        "    try:\n",
        "      info = yaml.safe_load(f)\n",
        "    except yaml.YAMLError as exc:\n",
        "      print(exc)\n",
        "      return\n",
        "\n",
        "  output_yaml_file = f\"{output_segmentation_directory}/data.yaml\"\n",
        "  with open(output_yaml_file, \"w\") as f:\n",
        "    yaml.dump(info, f, default_flow_style=False)\n",
        "\n",
        "  # the category names and root directory\n",
        "  # are specified in the yaml file\n",
        "  categories = info[\"names\"]\n",
        "\n",
        "  if 'path' in info:\n",
        "    dataset_root = f\"{object_detection_directory}/{info['path']}\"\n",
        "  else:\n",
        "    dataset_root = object_detection_directory\n",
        "\n",
        "  # these directories might\n",
        "  # have been set in the data.yaml file\n",
        "  possible_directories = [\"train\", \"val\", \"test\"]\n",
        "\n",
        "  # now we can start converting\n",
        "  # everything\n",
        "  for possible_dir in possible_directories:\n",
        "    input_images_directory = f\"{dataset_root}/{possible_dir}/images\"\n",
        "    input_labels_directory = \"/labels\".join(input_images_directory.rsplit(\"/images\", 1))\n",
        "\n",
        "    print(input_images_directory)\n",
        "    print(input_labels_directory)\n",
        "\n",
        "    if os.path.isdir(input_images_directory) and os.path.isdir(input_labels_directory):\n",
        "\n",
        "      output_images_directory = f\"{output_segmentation_directory}/{possible_dir}/images/\"\n",
        "      output_labels_directory = f\"{output_segmentation_directory}/{possible_dir}/labels/\"\n",
        "      print(f\"output {output_images_directory} {output_labels_directory}\")\n",
        "\n",
        "      # create needed directories\n",
        "      # if they don't exist yet\n",
        "      os.makedirs(output_images_directory, exist_ok=True)\n",
        "      os.makedirs(output_labels_directory, exist_ok=True)\n",
        "\n",
        "      # maybe we have already done some\n",
        "      # segmentations saved in the folder\n",
        "      # so we don't have to redo everything\n",
        "      existing_basenames = [os.path.basename(x) for x in glob.glob(f\"{output_images_directory}/*.jpg\")]\n",
        "      print(existing_basenames)\n",
        "\n",
        "      # do only segmentations for\n",
        "      # images that have not yet been written\n",
        "      # to the destination (even on second call of the function)\n",
        "      # because this function might interrupt due to time / resource limits\n",
        "      input_basenames = [os.path.basename(x) for x in glob.glob(f\"{input_images_directory}/*.jpg\")]\n",
        "      not_processed_images_basenames = sorted(list(set(input_basenames) - set(existing_basenames)))\n",
        "\n",
        "      print(f\"not processed: {not_processed_images_basenames}\")\n",
        "\n",
        "      for input_image_basename in not_processed_images_basenames:\n",
        "\n",
        "        # each input image will have\n",
        "        # a corresponding txt file\n",
        "        label_basename = os.path.splitext(input_image_basename)[0]  + \".txt\"\n",
        "\n",
        "        # now define the full output path\n",
        "        # for each file\n",
        "        output_image_path = f\"{output_images_directory}/{input_image_basename}\"\n",
        "        output_label_path = f\"{output_labels_directory}/{label_basename}\"\n",
        "        input_image_path = f\"{input_images_directory}/{input_image_basename}\"\n",
        "        input_label_path = f\"{input_labels_directory}/{label_basename}\"\n",
        "\n",
        "        print(f\"processing {input_image_path}\")\n",
        "\n",
        "        # load image and\n",
        "        # ensure that it is valid\n",
        "        image = cv2.imread(input_image_path)\n",
        "        if image is None:\n",
        "          continue\n",
        "\n",
        "        # load box annotations from\n",
        "        # the label file\n",
        "        results = load_segmentation_ground_truth(image.shape[:2], input_label_path, isbox=True)\n",
        "\n",
        "        boxes = np.array(results[\"boxes\"])\n",
        "        labels = results[\"labels\"]\n",
        "\n",
        "\n",
        "        if len(boxes) == 0:\n",
        "          print(\"no boxes\")\n",
        "\n",
        "        # convert bounding boxes\n",
        "        # to masks with Sam\n",
        "        masks = segment(\n",
        "          sam_predictor=sam_predictor,\n",
        "          image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB),\n",
        "          xyxy=boxes\n",
        "        )\n",
        "\n",
        "        if len(masks) == 0:\n",
        "          print(\"no masks\")\n",
        "\n",
        "        # optionally resize the image\n",
        "        if standard_size is not None:\n",
        "          image = cv2.resize(image, standard_size, interpolation = cv2.INTER_AREA)\n",
        "\n",
        "        label_content = []\n",
        "\n",
        "        image_height, image_width = image.shape[:2]\n",
        "        for mask, class_id in zip(masks, labels):\n",
        "\n",
        "            mask = mask.astype(dtype='uint8')\n",
        "            mask *= 255\n",
        "\n",
        "            # if the image was resized\n",
        "            # the masks need also be resized\n",
        "            if standard_size is not None:\n",
        "              mask = cv2.resize(mask, standard_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "            # now we need to convert the mask\n",
        "            # into a contour to save the result\n",
        "            # in the output label file\n",
        "            contours, hierarchy  = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            if contours is None or len(contours) == 0:\n",
        "              continue\n",
        "\n",
        "            # I am not expecting more than one contour\n",
        "            # but I want to make sure to get the right one\n",
        "            contour = max(contours, key = cv2.contourArea)\n",
        "\n",
        "            # yolov8 format expects the contour\n",
        "            # to be normalized\n",
        "            normalized_contour = contour / [image_width, image_height]\n",
        "            normalized_contour = normalized_contour.flatten()\n",
        "\n",
        "            contour_str = f\"{class_id} {' '.join(map(str, normalized_contour))}\"\n",
        "            label_content.append(contour_str)\n",
        "\n",
        "        content = '\\n'.join(label_content)\n",
        "        print(content)\n",
        "        # save the annotation\n",
        "        with open(output_label_path, 'w') as f:\n",
        "          f.write(content)\n",
        "\n",
        "        # save the file\n",
        "        cv2.imwrite(output_image_path, image)\n",
        "\n",
        "\n",
        "def split_cls(input_dir, output_dir, train_frac, val_frac):\n",
        "    if train_frac + val_frac > 1.00000001:\n",
        "        print(\"train val and test fractions must not exceed 1.0\")\n",
        "        return\n",
        "\n",
        "    input_dir = os.path.abspath(input_dir)\n",
        "    output_dir = os.path.abspath(output_dir)\n",
        "\n",
        "    # get any category names\n",
        "    # inside the input directory\n",
        "    categories = next(os.walk(input_dir))[1]\n",
        "\n",
        "\n",
        "    for category in categories:\n",
        "        image_paths = glob.glob(f\"{input_dir}/{category}/*.jpg\")\n",
        "        num_train = int(train_frac * len(image_paths))\n",
        "        num_val   = int(val_frac * len(image_paths))\n",
        "\n",
        "        num_test = len(image_paths) - num_train - num_val\n",
        "        print(f\"{category} {num_train} images for training\")\n",
        "        print(f\"{category} {num_val} images for validation\")\n",
        "        print(f\"{category} {num_test} images for testing\")\n",
        "\n",
        "        train_images = image_paths[0:num_train]\n",
        "        val_images = image_paths[num_train:(num_train+num_val)]\n",
        "        test_images = image_paths[(num_train+num_val):]\n",
        "\n",
        "        for image_path in train_images:\n",
        "            output_path = image_path.replace(input_dir, f\"{output_dir}/train\")\n",
        "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "            shutil.copyfile(image_path, output_path)\n",
        "\n",
        "        for image_path in val_images:\n",
        "            output_path = image_path.replace(input_dir, f\"{output_dir}/val\")\n",
        "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "            shutil.copyfile(image_path, output_path)\n",
        "\n",
        "        for image_path in test_images:\n",
        "            output_path = image_path.replace(input_dir, f\"{output_dir}/test\")\n",
        "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "            shutil.copyfile(image_path, output_path)\n",
        "\n",
        "\n",
        "def get_invalid_label_filenames(dataset_dir):\n",
        "  dataset_dir = os.path.abspath(dataset_dir)\n",
        "  label_filenames = glob.iglob(f\"{dataset_dir}/**/*.txt\", recursive=True)\n",
        "  invalid_label_filenames = []\n",
        "  for label_filename in label_filenames:\n",
        "    with open(label_filename, 'r') as f:\n",
        "      lines = f.read().splitlines()\n",
        "      for line in lines:\n",
        "          items = line.split(' ')\n",
        "          if len(items) > 0 and len(items) < 7:\n",
        "            invalid_label_filenames.append(label_filename)\n",
        "            print(label_filename)\n",
        "            break\n",
        "  return invalid_label_filenames\n",
        "\n",
        "def remove_invalid_label_filenames_and_images(invalid_label_filenames):\n",
        "  corresponding_image_filenames = []\n",
        "  for label_filename in invalid_label_filenames:\n",
        "    image_path = \"/images/\".join(label_filename.rsplit(\"/labels/\", 1))\n",
        "    image_path = \".jpg\".join(image_path.rsplit(\".txt\"))\n",
        "    corresponding_image_filenames.append(image_path)\n",
        "  for image_filename in corresponding_image_filenames:\n",
        "    if os.path.exists(image_filename):\n",
        "      print(f\"removing {image_filename}\")\n",
        "      os.remove(image_filename)\n",
        "  for label_filename in invalid_label_filenames:\n",
        "    if os.path.exists(label_filename):\n",
        "      print(f\"removing {label_filename}\")\n",
        "      os.remove(label_filename)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_XEjjjd3qHc"
      },
      "outputs": [],
      "source": [
        "seg_augment(\n",
        "        \"/content/drive/MyDrive/TrashBoxSegment\",\n",
        "        \"/content/drive/MyDrive/TrashBoxSegAug\",\n",
        "        get_simple_background_provider(\"/content/drive/MyDrive/conveyorbeltbackground/train/\"),\n",
        "        4,\n",
        "        image_prefix=\"trashboxsegaug_\"\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1tWpVV9_kPxNzvr2eU96TxoyiYCzMLBd1",
      "authorship_tag": "ABX9TyMK8jsee6UDAyaWG1DtH601",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}